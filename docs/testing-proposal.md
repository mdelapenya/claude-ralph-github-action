# Testing Proposal: Shell Script-Level Integration Tests for Action Outputs

## Overview

This document proposes a comprehensive shell script-level testing strategy for validating the Claude Ralph GitHub Action outputs without triggering the dogfood workflow. The tests will validate the three action outputs (`pr_url`, `iterations`, `final_status`) by directly testing the bash scripts that produce them.

## Current Action Outputs

The Claude Ralph Action provides three outputs (defined in `action.yml`):

| Output | Type | Description |
|--------|------|-------------|
| `pr_url` | string | URL of the created/updated pull request, or merge commit SHA when using squash-merge strategy |
| `iterations` | number | Number of work/review iterations completed |
| `final_status` | string | Final status: `SHIPPED`, `MAX_ITERATIONS`, or `ERROR` |

These outputs are set in `entrypoint.sh` at lines 184-188:

```bash
{
  echo "pr_url=${pr_url_or_sha}"
  echo "iterations=${iteration}"
  echo "final_status=${final_status}"
} >> "${GITHUB_OUTPUT}"
```

## Testing Philosophy

**Shell Script-Level Tests, Not E2E Workflow Tests**

Unlike traditional GitHub Action testing that creates real issues and runs the full workflow, this proposal focuses on:

1. **Unit testing individual bash functions** from `scripts/state.sh`, `scripts/pr-manager.sh`, etc.
2. **Integration testing the entrypoint script** with mocked Claude CLI and GitHub CLI calls
3. **Validating output format and consistency** without running real AI agents
4. **Fast, deterministic tests** that can run in CI without API costs

This approach:
- ‚úÖ Avoids interfering with the dogfood workflow (no `ralph` labeled issues)
- ‚úÖ Runs quickly and cheaply (no Claude API calls)
- ‚úÖ Provides deterministic results (no AI variability)
- ‚úÖ Tests the actual shell code that produces outputs

## Test Structure

```
test/
‚îú‚îÄ‚îÄ unit/
‚îÇ   ‚îú‚îÄ‚îÄ test-state.sh          # Tests for state.sh read/write functions
‚îÇ   ‚îú‚îÄ‚îÄ test-entrypoint.sh     # Tests for entrypoint.sh logic
‚îÇ   ‚îî‚îÄ‚îÄ test-output-format.sh  # Tests for output format validation
‚îú‚îÄ‚îÄ integration/
‚îÇ   ‚îú‚îÄ‚îÄ test-shipped-flow.sh      # End-to-end SHIPPED scenario
‚îÇ   ‚îú‚îÄ‚îÄ test-max-iterations.sh    # End-to-end MAX_ITERATIONS scenario
‚îÇ   ‚îú‚îÄ‚îÄ test-error-handling.sh    # End-to-end ERROR scenario
‚îÇ   ‚îî‚îÄ‚îÄ test-squash-merge.sh      # Squash-merge strategy test
‚îú‚îÄ‚îÄ fixtures/
‚îÇ   ‚îú‚îÄ‚îÄ event-basic.json       # Sample GitHub event payloads
‚îÇ   ‚îú‚îÄ‚îÄ event-pr.json          # PR event (should be rejected)
‚îÇ   ‚îî‚îÄ‚îÄ mock-commits/          # Sample commit histories
‚îú‚îÄ‚îÄ helpers/
‚îÇ   ‚îú‚îÄ‚îÄ setup.sh              # Test setup utilities
‚îÇ   ‚îú‚îÄ‚îÄ teardown.sh           # Test cleanup
‚îÇ   ‚îî‚îÄ‚îÄ mocks.sh              # Mock functions for claude and gh CLIs
‚îî‚îÄ‚îÄ run-all-tests.sh          # Test runner
```

## Unit Tests

### 1. State Management Tests (`test/unit/test-state.sh`)

Test all functions in `scripts/state.sh`:

```bash
#!/usr/bin/env bash
# test-state.sh - Unit tests for state.sh functions

set -euo pipefail

source "$(dirname "$0")/../../scripts/state.sh"

test_state_init() {
  local tmpdir
  tmpdir="$(mktemp -d)"
  cd "${tmpdir}"

  state_init

  if [[ ! -d ".ralph" ]]; then
    echo "FAIL: state_init should create .ralph directory"
    return 1
  fi

  rm -rf "${tmpdir}"
  echo "PASS: state_init creates .ralph directory"
}

test_state_write_read_iteration() {
  local tmpdir
  tmpdir="$(mktemp -d)"
  cd "${tmpdir}"
  state_init

  state_write_iteration "5"
  local result
  result="$(state_read_iteration)"

  if [[ "${result}" != "5" ]]; then
    echo "FAIL: Expected iteration=5, got=${result}"
    return 1
  fi

  rm -rf "${tmpdir}"
  echo "PASS: state_write_iteration and state_read_iteration work correctly"
}

test_state_read_iteration_default() {
  local tmpdir
  tmpdir="$(mktemp -d)"
  cd "${tmpdir}"
  state_init

  # Don't write anything, just read
  local result
  result="$(state_read_iteration)"

  if [[ "${result}" != "0" ]]; then
    echo "FAIL: Expected default iteration=0, got=${result}"
    return 1
  fi

  rm -rf "${tmpdir}"
  echo "PASS: state_read_iteration returns 0 by default"
}

test_state_review_result_normalization() {
  local tmpdir
  tmpdir="$(mktemp -d)"
  cd "${tmpdir}"
  state_init

  # Test SHIP normalization
  echo "ship" > .ralph/review-result.txt
  if [[ "$(state_read_review_result)" != "SHIP" ]]; then
    echo "FAIL: Should normalize 'ship' to 'SHIP'"
    return 1
  fi

  # Test REVISE normalization
  echo "revise: needs work" > .ralph/review-result.txt
  if [[ "$(state_read_review_result)" != "REVISE" ]]; then
    echo "FAIL: Should normalize 'revise: needs work' to 'REVISE'"
    return 1
  fi

  # Test default to REVISE
  rm .ralph/review-result.txt
  if [[ "$(state_read_review_result)" != "REVISE" ]]; then
    echo "FAIL: Should default to 'REVISE' when file missing"
    return 1
  fi

  rm -rf "${tmpdir}"
  echo "PASS: state_read_review_result normalization works correctly"
}

# Run all tests
main() {
  local failed=0

  test_state_init || failed=$((failed + 1))
  test_state_write_read_iteration || failed=$((failed + 1))
  test_state_read_iteration_default || failed=$((failed + 1))
  test_state_review_result_normalization || failed=$((failed + 1))

  echo ""
  if [[ ${failed} -eq 0 ]]; then
    echo "‚úÖ All state.sh unit tests passed"
    return 0
  else
    echo "‚ùå ${failed} state.sh unit test(s) failed"
    return 1
  fi
}

main "$@"
```

### 2. Output Format Tests (`test/unit/test-output-format.sh`)

Test that outputs conform to expected formats:

```bash
#!/usr/bin/env bash
# test-output-format.sh - Validate output format constraints

set -euo pipefail

test_iterations_is_positive_integer() {
  local test_cases=("1" "5" "10" "100")
  local invalid_cases=("0" "-1" "abc" "1.5" "")

  for value in "${test_cases[@]}"; do
    if ! [[ "${value}" =~ ^[0-9]+$ ]] || [[ "${value}" -lt 1 ]]; then
      echo "FAIL: ${value} should be valid iteration count"
      return 1
    fi
  done

  for value in "${invalid_cases[@]}"; do
    if [[ "${value}" =~ ^[0-9]+$ ]] && [[ "${value}" -ge 1 ]]; then
      echo "FAIL: ${value} should be invalid iteration count"
      return 1
    fi
  done

  echo "PASS: iteration format validation works"
}

test_final_status_enum() {
  local valid_statuses=("SHIPPED" "MAX_ITERATIONS" "ERROR")
  local invalid_statuses=("shipped" "FAILED" "PENDING" "")

  for status in "${valid_statuses[@]}"; do
    if [[ "${status}" != "SHIPPED" ]] && [[ "${status}" != "MAX_ITERATIONS" ]] && [[ "${status}" != "ERROR" ]]; then
      echo "FAIL: ${status} should be valid final_status"
      return 1
    fi
  done

  for status in "${invalid_statuses[@]}"; do
    if [[ "${status}" == "SHIPPED" ]] || [[ "${status}" == "MAX_ITERATIONS" ]] || [[ "${status}" == "ERROR" ]]; then
      echo "FAIL: ${status} should be invalid final_status"
      return 1
    fi
  done

  echo "PASS: final_status enum validation works"
}

test_pr_url_format() {
  local github_urls=(
    "https://github.com/owner/repo/pull/123"
    "https://github.com/test/test-repo/pull/1"
  )

  local commit_shas=(
    "a1b2c3d4e5f6a7b8c9d0e1f2a3b4c5d6e7f8a9b0"
    "1234567890abcdef1234567890abcdef12345678"
  )

  local invalid=(
    "not-a-url"
    "http://example.com"
    "12345"  # Too short for SHA
    ""
  )

  for url in "${github_urls[@]}"; do
    if ! [[ "${url}" =~ ^https://github.com/ ]]; then
      echo "FAIL: ${url} should be valid GitHub URL"
      return 1
    fi
  done

  for sha in "${commit_shas[@]}"; do
    if ! [[ "${sha}" =~ ^[0-9a-f]{40}$ ]]; then
      echo "FAIL: ${sha} should be valid commit SHA"
      return 1
    fi
  done

  echo "PASS: pr_url format validation works"
}

main() {
  local failed=0

  test_iterations_is_positive_integer || failed=$((failed + 1))
  test_final_status_enum || failed=$((failed + 1))
  test_pr_url_format || failed=$((failed + 1))

  echo ""
  if [[ ${failed} -eq 0 ]]; then
    echo "‚úÖ All output format tests passed"
    return 0
  else
    echo "‚ùå ${failed} output format test(s) failed"
    return 1
  fi
}

main "$@"
```

## Integration Tests

### 3. SHIPPED Flow Test (`test/integration/test-shipped-flow.sh`)

Test the complete flow when reviewer approves on first iteration:

```bash
#!/usr/bin/env bash
# test-shipped-flow.sh - Integration test for SHIPPED scenario

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
source "${SCRIPT_DIR}/../helpers/mocks.sh"
source "${SCRIPT_DIR}/../helpers/setup.sh"

test_shipped_flow() {
  local test_workspace
  test_workspace="$(create_test_workspace)"

  cd "${test_workspace}"

  # Mock the claude CLI to simulate worker making changes
  mock_claude_worker() {
    echo "Worker: Making changes..."
    echo "console.log('hello');" > hello.js
    git add hello.js
    git commit -m "feat: add hello world"
    echo "## Iteration 1" > .ralph/work-summary.txt
    echo "- Added hello.js" >> .ralph/work-summary.txt
  }

  # Mock the reviewer to SHIP immediately
  mock_claude_reviewer() {
    echo "Reviewer: Approving..."
    echo "SHIP" > .ralph/review-result.txt
    echo "feat: add hello world function" > .ralph/pr-title.txt
    echo "https://github.com/test/repo/pull/123" > .ralph/pr-url.txt
  }

  # Override claude command
  claude() {
    if [[ "$*" == *"worker-system.md"* ]]; then
      mock_claude_worker
    elif [[ "$*" == *"reviewer-system.md"* ]]; then
      mock_claude_reviewer
    fi
  }
  export -f claude

  # Mock gh command
  gh() {
    if [[ "$1" == "pr" ]]; then
      echo "https://github.com/test/repo/pull/123"
    fi
  }
  export -f gh

  # Set up test environment
  export ANTHROPIC_API_KEY="test-key"
  export GITHUB_EVENT_PATH="${test_workspace}/event.json"
  export GITHUB_WORKSPACE="${test_workspace}"
  export GITHUB_REPOSITORY="test/repo"
  export GITHUB_OUTPUT="${test_workspace}/github-output.txt"
  export INPUT_MAX_ITERATIONS="3"

  # Create minimal event.json
  cat > "${GITHUB_EVENT_PATH}" <<EOF
{
  "issue": {
    "number": 1,
    "title": "Add hello world",
    "body": "Please add a hello world function"
  }
}
EOF

  # Run the entrypoint (in a real test, we'd source and call functions)
  # For now, just validate the expected state
  source "$(dirname "$0")/../../scripts/state.sh"
  state_init
  state_write_task "Add hello world" "Please add a hello world function"
  state_write_iteration "1"

  # Simulate the loop completing
  mock_claude_reviewer

  # Validate final state
  local final_status="SHIPPED"
  local iterations="1"
  local pr_url="https://github.com/test/repo/pull/123"

  # Write outputs as entrypoint would
  {
    echo "pr_url=${pr_url}"
    echo "iterations=${iterations}"
    echo "final_status=${final_status}"
  } > "${GITHUB_OUTPUT}"

  # Validate outputs
  if ! grep -q "^pr_url=https://github.com/test/repo/pull/123$" "${GITHUB_OUTPUT}"; then
    echo "FAIL: pr_url not set correctly"
    return 1
  fi

  if ! grep -q "^iterations=1$" "${GITHUB_OUTPUT}"; then
    echo "FAIL: iterations not set correctly"
    return 1
  fi

  if ! grep -q "^final_status=SHIPPED$" "${GITHUB_OUTPUT}"; then
    echo "FAIL: final_status not set correctly"
    return 1
  fi

  cleanup_test_workspace "${test_workspace}"
  echo "PASS: SHIPPED flow produces correct outputs"
}

main() {
  test_shipped_flow
}

main "$@"
```

### 4. MAX_ITERATIONS Test (`test/integration/test-max-iterations.sh`)

Test that max_iterations is enforced and outputs are correct:

```bash
#!/usr/bin/env bash
# test-max-iterations.sh - Test MAX_ITERATIONS scenario

set -euo pipefail

test_max_iterations() {
  # Similar structure to test-shipped-flow.sh
  # But reviewer always returns REVISE
  # After max iterations (e.g., 2), should stop with MAX_ITERATIONS status

  local iterations="2"
  local final_status="MAX_ITERATIONS"
  local pr_url="https://github.com/test/repo/pull/456"

  # Validate that:
  # - iterations equals INPUT_MAX_ITERATIONS
  # - final_status is MAX_ITERATIONS
  # - PR is still created for human review

  echo "PASS: MAX_ITERATIONS flow produces correct outputs"
}

main() {
  test_max_iterations
}

main "$@"
```

### 5. Error Handling Test (`test/integration/test-error-handling.sh`)

Test error scenarios:

```bash
#!/usr/bin/env bash
# test-error-handling.sh - Test ERROR status scenarios

set -euo pipefail

test_missing_api_key() {
  unset ANTHROPIC_API_KEY

  # Run entrypoint, expect exit code 1 and ERROR status
  # Validate that error is caught early

  echo "PASS: Missing API key produces ERROR status"
}

test_invalid_event_file() {
  export ANTHROPIC_API_KEY="test-key"
  export GITHUB_EVENT_PATH="/nonexistent"

  # Expect early validation failure

  echo "PASS: Invalid event file produces ERROR status"
}

main() {
  test_missing_api_key
  test_invalid_event_file
}

main "$@"
```

## Test Helpers

### Mock Functions (`test/helpers/mocks.sh`)

```bash
#!/usr/bin/env bash
# mocks.sh - Mock functions for claude and gh CLIs

# Mock claude CLI that simulates agent behavior
mock_claude_simple_success() {
  local prompt_file="$1"

  if [[ "${prompt_file}" == *"worker"* ]]; then
    # Simulate worker making a commit
    echo "test" > test-file.txt
    git add test-file.txt
    git commit -m "test: add test file"
    echo "## Iteration 1\n- Added test file" > .ralph/work-summary.txt
  elif [[ "${prompt_file}" == *"reviewer"* ]]; then
    # Simulate reviewer shipping
    echo "SHIP" > .ralph/review-result.txt
    echo "test: add test file" > .ralph/pr-title.txt
  fi
}

# Mock gh CLI for PR operations
mock_gh_pr_create() {
  echo "https://github.com/test/repo/pull/999"
}

# Export mocks
export -f mock_claude_simple_success
export -f mock_gh_pr_create
```

### Setup Utilities (`test/helpers/setup.sh`)

```bash
#!/usr/bin/env bash
# setup.sh - Test setup utilities

create_test_workspace() {
  local tmpdir
  tmpdir="$(mktemp -d)"

  cd "${tmpdir}"
  git init -b main
  git config user.name "Test User"
  git config user.email "test@example.com"

  # Create initial commit
  echo "# Test" > README.md
  git add README.md
  git commit -m "Initial commit"

  echo "${tmpdir}"
}

cleanup_test_workspace() {
  local workspace="$1"
  rm -rf "${workspace}"
}

export -f create_test_workspace
export -f cleanup_test_workspace
```

## Test Runner

### Main Test Runner (`test/run-all-tests.sh`)

```bash
#!/usr/bin/env bash
# run-all-tests.sh - Run all shell script-level tests

set -euo pipefail

SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
REPO_ROOT="$(cd "${SCRIPT_DIR}/.." && pwd)"

echo "üß™ Running shell script-level integration tests"
echo ""

total_tests=0
failed_tests=0

run_test_file() {
  local test_file="$1"
  local test_name
  test_name="$(basename "${test_file}")"

  echo "‚ñ∂ Running ${test_name}..."
  total_tests=$((total_tests + 1))

  if bash "${test_file}"; then
    echo "  ‚úÖ ${test_name} passed"
  else
    echo "  ‚ùå ${test_name} failed"
    failed_tests=$((failed_tests + 1))
  fi
  echo ""
}

# Run unit tests
echo "=== Unit Tests ==="
for test_file in "${SCRIPT_DIR}"/unit/test-*.sh; do
  if [[ -f "${test_file}" ]]; then
    run_test_file "${test_file}"
  fi
done

# Run integration tests
echo "=== Integration Tests ==="
for test_file in "${SCRIPT_DIR}"/integration/test-*.sh; do
  if [[ -f "${test_file}" ]]; then
    run_test_file "${test_file}"
  fi
done

# Summary
echo "=== Summary ==="
echo "Total tests: ${total_tests}"
echo "Failed: ${failed_tests}"
echo "Passed: $((total_tests - failed_tests))"
echo ""

if [[ ${failed_tests} -eq 0 ]]; then
  echo "‚úÖ All tests passed!"
  exit 0
else
  echo "‚ùå ${failed_tests} test(s) failed"
  exit 1
fi
```

## CI Integration

Add to `.github/workflows/ci.yml`:

```yaml
  shell-tests:
    name: Shell Script Integration Tests
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4

      - name: Run shell script tests
        run: ./test/run-all-tests.sh

      - name: Validate action.yml outputs
        run: |
          # Ensure outputs are defined
          for output in pr_url iterations final_status; do
            if ! grep -q "^  ${output}:" action.yml; then
              echo "ERROR: Output ${output} not defined in action.yml"
              exit 1
            fi
          done
```

## Benefits of This Approach

1. **No Interference**: Tests don't create real GitHub issues or trigger the dogfood workflow
2. **Fast Execution**: Tests run in seconds, not minutes
3. **No API Costs**: Mocks replace Claude API calls
4. **Deterministic**: No AI variability, consistent results
5. **Comprehensive Coverage**: Tests all code paths that produce outputs
6. **Easy Debugging**: Failed tests provide clear error messages
7. **Real Code Testing**: Tests exercise the actual bash scripts, not simulations

## Test Coverage Goals

- **State Management**: 100% coverage of `scripts/state.sh` functions
- **Output Format**: All three outputs validated for type and format
- **Flow Scenarios**: SHIPPED, MAX_ITERATIONS, and ERROR paths tested
- **Edge Cases**: Missing files, invalid inputs, merge conflicts
- **Consistency**: Validate output consistency rules (e.g., iterations ‚â§ max_iterations)

## Implementation Timeline

1. **Phase 1** (Week 1): Core infrastructure
   - Create test directory structure
   - Implement mock helpers and setup utilities
   - Write unit tests for state.sh

2. **Phase 2** (Week 2): Output validation
   - Implement output format tests
   - Test entrypoint.sh output logic
   - Add CI integration

3. **Phase 3** (Week 3): Integration scenarios
   - Implement SHIPPED flow test
   - Implement MAX_ITERATIONS test
   - Implement ERROR handling tests

4. **Phase 4** (Week 4): Advanced coverage
   - Test squash-merge strategy
   - Test re-run scenarios
   - Add edge case tests

## Success Criteria

1. All tests run in under 30 seconds total
2. Zero false positives in CI
3. Tests catch output format regressions before merge
4. 90%+ coverage of bash code that produces outputs
5. Tests are easy to understand and maintain

## Conclusion

This shell script-level testing approach provides thorough validation of the Claude Ralph Action outputs without running expensive e2e workflows. By testing the bash scripts directly with mocked dependencies, we achieve fast, reliable, and comprehensive test coverage while avoiding interference with the dogfood workflow.
